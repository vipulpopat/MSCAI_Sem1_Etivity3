{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E-tivity 3: Linear classification\n",
    "## MSc in AI - CE6002 & CS5062\n",
    "Student Name: Michel Danjou\n",
    "\n",
    "Student ID: 18263461\n",
    "\n",
    "<span style=\"color:red\">TODO: Add student id to filename.</span>  \n",
    "<span style=\"color:red\">TODO: Add labels to all plots.</span>  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets.samples_generator import make_circles\n",
    "X, y = make_circles(n_samples=1000, noise = 0.05)\n",
    "y = [yy if yy == 1 else -1 for yy in y] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3 (CE6002)\n",
    "#### *Create your own implementation of linear classification to perform a classification of the dataset provided in the Etivity3_LinearClassification.ipynb notebook without adding extra features to those provided. Use normal linear regression with sign(wTx) to obtain a classification. Notebook Etivity3_LinearClassification.ipynb is available in the git repository. Add your code to this notebook.*\n",
    " \n",
    "Interesting reads: \n",
    "  * https://machinelearningmastery.com/solve-linear-regression-using-linear-algebra/\n",
    "  * https://hadrienj.github.io/posts/Deep-Learning-Book-Series-2.9-The-Moore-Penrose-Pseudoinverse/ (homemade linear regression algo)\n",
    "  * https://www.datacamp.com/community/tutorials/understanding-logistic-regression-python (confusion matrix)\n",
    "  \n",
    "<span style=\"color:red\">TODO: Split between training and testing data.</span>  \n",
    "<span style=\"color:red\">TODO: Calculate Mean Squared Error.</span>  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xcopy = np.copy(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the boundary\n",
    "def decision_boundary_lin(weights, x_min, x_max):\n",
    "   # Return two points on the decision boundary\n",
    "   return [point_on_boundary_lin(weights, x_min), point_on_boundary_lin(weights, x_max)]\n",
    "\n",
    "def point_on_boundary_lin(weights, x):\n",
    "    # Return the y-position on the boundary based on given x-position\n",
    "    return -(weights[0]+weights[1]*x)/weights[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression(X, y):\n",
    "    X_inverse = np.linalg.pinv(X)\n",
    "    w_lin = np.matmul(X_inverse, y)\n",
    "    return w_lin\n",
    "\n",
    "\n",
    "def calc_square_error(a, b):\n",
    "    e_sq = 0\n",
    "    N = len(a)\n",
    "    for i in range(N):\n",
    "        e_sq += math.pow((a[i] - b[i]),2)\n",
    "    return math.sqrt(e_sq/N)\n",
    "\n",
    "\n",
    "\n",
    "def calculate_and_plot_linear_regression(X, y):\n",
    "    \n",
    "    # Insert bias\n",
    "    X = np.insert(X, 0, 1, axis=1)\n",
    "\n",
    "    # shuffle data\n",
    "    X, y = shuffle(X, y, random_state=0)\n",
    "\n",
    "    # split data\n",
    "    #X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    X_train=X[100:]\n",
    "    y_train=y[100:]\n",
    "    X_test=X[:100]\n",
    "    y_test=y[:100]\n",
    "\n",
    "    # Linear regression\n",
    "    weights = linear_regression(X_train, y_train)\n",
    "    \n",
    "    # Plot test data\n",
    "    plt.scatter(X[:,1], X[:,2], c=y)\n",
    "\n",
    "    # Plot\n",
    "    minx = np.amin(X)\n",
    "    maxx = np.amax(X)\n",
    "    miny, maxy = decision_boundary_lin(weights, minx, maxx)\n",
    "    plt.plot([minx, maxx], [miny, maxy], 'ro-')\n",
    "    \n",
    "    # yhat (multiply yhat by 2 so that, when plotted, it doesn't overlap with scatter plot of X)\n",
    "    yhat = X_test.dot(weights)\n",
    "    plt.scatter(X_test[:,1], np.sign(yhat)*2, c=y_test)\n",
    "    \n",
    "    # error\n",
    "    error = calc_square_error(y_test,  yhat)\n",
    "    \n",
    "    # Marker for center point at coordinates (0,0)\n",
    "    plt.plot([0], [0], marker='x', markersize=30, color=\"red\")\n",
    "\n",
    "    \n",
    "    return weights, yhat, error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT:** \n",
    "  * If we train the linear regression on the **entire data** set then the generated **boundary line crosses the origin**.\n",
    "  * If we **split the data set** between the training and testing data set, then the corresponding **boundary line does not cross the origin**. This is counter intuitive as it seem there are enought samples for the algo to correctly estimate the weights. Maybe there is a **bug** the in function used to draw the boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.copy(Xcopy)\n",
    "\n",
    "weights, yhat, error = calculate_and_plot_linear_regression(X, y)\n",
    "print(\"Error:\", error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Observe your results and explain why these results seem disappointing (record your thoughts in a Markdown cell in your notebook).*\n",
    "\n",
    "The demarcation mark between the 2 sets of data is correct as it evenly splits both values representing each y values. However it is of no real value.\n",
    "\n",
    "*TODO*: Calculate the error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Now choose suitable new features and use these in your linear regression algorithm to improve the classification performance. Observe and explain (use plots where appropriate) why the classification performance has improved. Try a few different ones and note the differences!*\n",
    "\n",
    "Interesting reads: \n",
    "  *   https://www.deeplearningbook.org/contents/ml.html (page 110, figure 5.16)\n",
    "  \n",
    "##### Square transformation  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.copy(Xcopy)\n",
    "X = np.square(X)\n",
    "\n",
    "weights, yhat, error = calculate_and_plot_linear_regression(X, y)\n",
    "print(\"Error:\", error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cubic transformation\n",
    "We can see that for the cubic transformation, the error is has high as with the original data set. This is expected as the cubic transformation preserves the sign of X which results is a distribution pattern centered around the origin in our case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.copy(Xcopy)\n",
    "X = np.power(X, 3)\n",
    "\n",
    "weights, yhat, error = calculate_and_plot_linear_regression(X, y)\n",
    "print(\"Error:\", error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Power 4 transformation\n",
    "The error is higher than for the power of 2 transformation. This is counter intuitive as I would have expected the error to remain the same. \n",
    "\n",
    "<span style=\"color:red\">This leads me to suspect that there is a bug in the error function.</span>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.copy(Xcopy)\n",
    "X = np.power(X, 4)\n",
    "\n",
    "weights, yhat, error = calculate_and_plot_linear_regression(X, y)\n",
    "print(\"Error:\", error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Trigonometric transformations\n",
    "Out of curiosity I wanted to see the impact of a trigonometric function on the Linear classification, instead of the \n",
    "polynomial transformation.\n",
    "\n",
    "We can see that cos() gives an error rate similar to square(). This doesn't mean that, in general, cos() is a good as square(). The fact that the error is as good as square() probably boils down to the original method for creating the sample data. By chance, none of the data would generate a negative value when passed to the cosinus function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.copy(Xcopy)\n",
    "X = np.cos(X)\n",
    "\n",
    "weights, yhat, error = calculate_and_plot_linear_regression(X, y)\n",
    "print(\"Error:\", error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.copy(Xcopy)\n",
    "X = np.sin(X)\n",
    "\n",
    "weights, yhat, error = calculate_and_plot_linear_regression(X, y)\n",
    "print(\"Error:\", error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Also, use scikit-learnâ€™s Logistic Regression algorithm and compare the performance with your algorithm. It is useful to spend some time thinking about the difference in approach taken in Logistic Regression.*\n",
    "\n",
    "Interesting links: \n",
    "  * https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n",
    "  * https://stackoverflow.com/questions/28256058/plotting-decision-boundary-of-logistic-regression\n",
    "  * https://scikit-learn.org/stable/auto_examples/linear_model/plot_iris_logistic.html\n",
    " \n",
    "<span style=\"color:red\">TODO: Plot data set with decision boundary line.</span>  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_scikit(X, Y):\n",
    "    color_map = 'Pastel1'\n",
    "    logreg = LogisticRegression(C=1e5, solver='lbfgs', multi_class='multinomial')\n",
    "    logreg.fit(X, Y)\n",
    "\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    h = .02  # step size in the mesh\n",
    "\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    Z = logreg.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.figure(1, figsize=(4, 3))\n",
    "    plt.pcolormesh(xx, yy, Z)\n",
    "\n",
    "    # Plot also the training points\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=Y, alpha=0.8, edgecolors='w')\n",
    "\n",
    "\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Logistic regression on X\n",
    "X = np.copy(Xcopy)\n",
    "logistic_regression_scikit(X, y)\n",
    "\n",
    "# Logistic regression on X square\n",
    "logistic_regression_scikit(np.square(X), y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backup work beyond this point. Do not review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add bias=1 column\n",
    "X = np.insert(X, 0, 1, axis=1)\n",
    "\n",
    "# Calculate pseudo-inverse\n",
    "X_inverse = np.linalg.pinv(X)\n",
    "w_lin = np.matmul(X_inverse, y)\n",
    "w_lin_dot = X_inverse.dot(y)\n",
    "\n",
    "minx = np.amin(X)\n",
    "maxx = np.amax(X)\n",
    "\n",
    "# Plot the data set\n",
    "plt.scatter(X[:,1], X[:,2], c=y)\n",
    "\n",
    "# Plot the decision boundary\n",
    "minx = np.amin(X)\n",
    "maxx = np.amax(X)\n",
    "miny, maxy = decision_boundary_lin(w_lin, minx, maxx)\n",
    "plt.plot([minx, maxx], [miny, maxy], 'ro-')\n",
    "\n",
    "# Alternative decision boundary\n",
    "#horz = np.linspace(minx, maxx, 1000)\n",
    "#vert = w_lin[0]*X + w_lin[1]\n",
    "#plt.plot(horz, vert, color='blue')\n",
    "\n",
    "# Alternative approach for drawing the separating line.\n",
    "#yhat = X.dot(w_lin)\n",
    "#plt.plot(X, yhat, color='red')\n",
    "#plt.plot(X, np.sign(yhat), color='red')\n",
    "#plt.plot(X, y, color='red')\n",
    "#sign = np.sign(yhat)\n",
    "\n",
    "# Debug\n",
    "print(\"X.shape        :{}\".format(X.shape))\n",
    "print(\"y.len          :{}\".format(len(y)))\n",
    "print(\"X_inverse      :{}\".format(X_inverse ))\n",
    "print(\"X_inverse.shape:{}\".format(X_inverse.shape ))\n",
    "print(\"w_lin          :{}\".format(w_lin))\n",
    "print(\"w_lin_dot      :{}\".format(w_lin_dot))\n",
    "print(\"minx           :\", minx)\n",
    "print(\"maxx           :\", maxx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.copy(Xcopy)\n",
    "\n",
    "# Square the data\n",
    "Xsq = np.square(X)\n",
    "\n",
    "# Add bias=1 column\n",
    "X = np.insert(Xsq, 0, 1, axis=1)\n",
    "\n",
    "# Calculate pseudo-inverse\n",
    "X_inverse = np.linalg.pinv(X)\n",
    "w_lin = np.matmul(X_inverse, y)\n",
    "w_lin_dot = X_inverse.dot(y)\n",
    "\n",
    "# Plot the data set\n",
    "plt.scatter(X[:,1], X[:,2], c=y)\n",
    "\n",
    "# Plot the decision boundary\n",
    "minx = np.amin(Xsq)\n",
    "maxx = np.amax(Xsq)\n",
    "miny, maxy = decision_boundary_lin(w_lin, minx, maxx)\n",
    "plt.plot([minx, maxx], [miny, maxy], 'ro-')\n",
    "\n",
    "# Debug - Remove\n",
    "print(\"X.shape        :{}\".format(X.shape))\n",
    "print(\"y.len          :{}\".format(len(y)))\n",
    "print(\"X_inverse      :{}\".format(X_inverse ))\n",
    "print(\"X_inverse.shape:{}\".format(X_inverse.shape ))\n",
    "print(\"w_lin          :{}\".format(w_lin))\n",
    "print(\"w_lin_dot      :{}\".format(w_lin_dot))\n",
    "print(\"minx           :\", minx)\n",
    "print(\"maxx           :\", maxx)\n",
    "print(\"miny           :\", miny)\n",
    "print(\"maxy           :\", maxy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression(X, y):\n",
    "    X_inverse = np.linalg.pinv(X)\n",
    "    w_lin = np.matmul(X_inverse, y)\n",
    "    return w_lin\n",
    "\n",
    "\n",
    "def calc_square_error(a, b):\n",
    "    e_sq = 0\n",
    "    N = len(a)\n",
    "    for i in range(N):\n",
    "        e_sq += math.pow((a[i] - b[i]),2)\n",
    "    return math.sqrt(e_sq/N)\n",
    "\n",
    "\n",
    "def calculate_and_plot_linear_regression(X, y):\n",
    "    \n",
    "    # Insert bias\n",
    "    X = np.insert(X, 0, 1, axis=1)\n",
    "\n",
    "    # Linear regression\n",
    "    weights = linear_regression(X, y)\n",
    "    yhat = X.dot(weights)\n",
    "    plt.scatter(X[:,1], X[:,2], c=y)\n",
    "\n",
    "    # Plot\n",
    "    minx = np.amin(X)\n",
    "    maxx = np.amax(X)\n",
    "    miny, maxy = decision_boundary_lin(weights, minx, maxx)\n",
    "    plt.plot([minx, maxx], [miny, maxy], 'ro-')\n",
    "    \n",
    "    # yhat (multiply yhat by 2 so that, when plotted, it doesn't overlap with scatter plot of X)\n",
    "    plt.scatter(X[:,1], np.sign(yhat)*2, c=y)\n",
    "    \n",
    "    # error\n",
    "    error = calc_square_error(y,  yhat)\n",
    "    \n",
    "    return weights, yhat, error\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets.samples_generator import make_circles\n",
    "X, y = make_circles(n_samples=1000, noise = 0.05)\n",
    "y = [yy if yy == 1 else -1 for yy in y]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_train[:,0],X_train[:,1], c=y_train, cmap='seismic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thanks to Michel Danjou and Abhijit Sinha \n",
    "# for the examples which enabled me to find my mistakes in my implementation\n",
    "\n",
    "class LinearClassifier:\n",
    "    def fit(self, X, y):\n",
    "        X = np.insert(X, 0, 1, axis=1)        \n",
    "        pseudo_inv_matrix = np.linalg.pinv(X)\n",
    "        weight_matrix = pseudo_inv_matrix.dot(y)\n",
    "        self.w = weight_matrix\n",
    "        \n",
    "    def calculate_error(self, predictions, true):\n",
    "        errors = 0\n",
    "        for x,y in zip(predictions, true):\n",
    "            if (x !=y):\n",
    "                errors+=1\n",
    "        return errors / len(predictions)\n",
    "\n",
    "    def predict_class(self, X):\n",
    "        X = np.insert(X, 0, 1, axis=1)\n",
    "        # Function to predict the classification label for the input data X\n",
    "        return np.sign(np.dot(X, np.transpose(self.w)))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return -(self.w[0]+self.w[1]*X)/self.w[2]\n",
    "    \n",
    "    def plot(self, X, Y, title=\"\"):\n",
    "        X = np.insert(X, 0, 1, axis=1)\n",
    "        plt.scatter(X[:,1],X[:,2], c=Y, cmap='seismic')\n",
    "        minx = X.min()\n",
    "        maxx = X.max()\n",
    "        miny = self.predict(minx) \n",
    "        maxy = self.predict(maxx) \n",
    "        plt.title(title)\n",
    "        plt.plot([minx, maxx], [miny, maxy], color='blue')\n",
    "        plt.show()\n",
    "\n",
    "clf = LinearClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "clf.plot(X_train, y_train, \"y_train\")\n",
    "clf.plot(X_test, y_test, \"y_test\")\n",
    "pred = clf.predict_class(X_test)\n",
    "error = clf.calculate_error(pred, y_test)\n",
    "print(\"Error Out:\", error)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The e Out is quite high (.459) is dissapointing as the dataset is not linearly seperable. We will attempt to improve the accuracy score by creating a new feature by applying a linear transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2_train = X_train * X_train\n",
    "X2_test = X_test * X_test\n",
    "clf2 = LinearClassifier()\n",
    "clf2.fit(X2_train, y_train)\n",
    "clf2.plot(X2_train, y_train)\n",
    "clf2.plot(X2_test, y_test)\n",
    "pred = clf2.predict_class(X2_test)\n",
    "error = clf2.calculate_error(pred, y_test)\n",
    "print(\"Error Out:\", error)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a much better score, however we cannot 'guarantee' that this will generalise well as we have broken the VC bound by data snooping. I will complete some more transformations now to compare. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X3_train = X_train * X_train * X_train\n",
    "X3_test = X_test * X_test * X_test\n",
    "\n",
    "clf3 = LinearClassifier()\n",
    "clf3.fit(X3_train, y_train)\n",
    "clf3.plot(X3_train, y_train)\n",
    "clf3.plot(X3_test, y_test)\n",
    "pred = clf2.predict_class(X3_test)\n",
    "error = clf2.calculate_error(pred, y_test)\n",
    "print(\"Error Out:\", error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X3_train = X_train * X_train  * X_train * X_train\n",
    "X3_test = X_test * X_test * X_test * X_test\n",
    "\n",
    "clf3 = LinearClassifier()\n",
    "clf3.fit(X3_train, y_train)\n",
    "clf3.plot(X3_train, y_train)\n",
    "clf3.plot(X3_test, y_test)\n",
    "pred = clf2.predict_class(X3_test)\n",
    "error = clf2.calculate_error(pred, y_test)\n",
    "print(\"Error Out:\", error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X3_train = np.absolute(X_train)\n",
    "X3_test = np.absolute(X_test)\n",
    "\n",
    "clf3 = LinearClassifier()\n",
    "clf3.fit(X3_train, y_train)\n",
    "clf3.plot(X3_train, y_train)\n",
    "clf3.plot(X3_test, y_test)\n",
    "pred = clf3.predict_class(X3_test)\n",
    "error = clf3.calculate_error(pred, y_test)\n",
    "print(\"Error Out:\", error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X^2 appears to give the best error with our implementation of linear classification. We can see that the absolute values element wise of X also appears to give a linearly seperable dataset however our error is much higher. \n",
    "\n",
    "\n",
    "We will now compare results of Sklearn Logistic regression with both these datasets. I suspect that it will perform similarly on both dataset as the linear seperator looks pretty good on both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression()\n",
    "clf.fit(X2_train, y_train)\n",
    "y_pred = clf.predict(X2_test)\n",
    "plt.scatter(X_test[:,0],X_test[:,1], c=y_pred, cmap='seismic')\n",
    "plt.show()\n",
    "print(accuracy_score(y_pred, y_test))\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X3_train, y_train)\n",
    "y_pred = clf.predict(X3_test)\n",
    "plt.scatter(X_test[:,0],X_test[:,1], c=y_pred, cmap='seismic')\n",
    "plt.show()\n",
    "print(accuracy_score(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implemented linear regression works just as well as the out of the box logistic regression on the linearly seperable datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
